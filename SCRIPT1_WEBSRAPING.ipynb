{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mqJb9J2c-3E"
      },
      "outputs": [],
      "source": [
        "import json, requests\n",
        "import html5lib\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_part(web_page,class_title):\n",
        "  result=''\n",
        "  page = requests.get(web_page)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "\n",
        "  # Convert the list element to a string\n",
        "  div_element=soup.find_all('div', class_=class_title)\n",
        "  html = str(div_element)\n",
        "\n",
        "  # Use regular expressions to find headers and their content\n",
        "  pattern = r'<h\\d>(.*?)<\\/h\\d>(.*?)((?=<h\\d>)|$)'\n",
        "  matches = re.findall(pattern, html, re.DOTALL)\n",
        "\n",
        "  # Extract headers and content\n",
        "  headers = []\n",
        "  content = []\n",
        "\n",
        "  for match in matches:\n",
        "    headers.append(match[0])\n",
        "    content.append(match[1])\n",
        "\n",
        "  # Print the headers and their corresponding content\n",
        "  for i in range(len(set(headers))):\n",
        "    soup2 = BeautifulSoup(content[i], 'html.parser')\n",
        "    result+=headers[i]+':'+'\\n'+soup2.get_text()+'\\n'\n",
        "  return result"
      ],
      "metadata": {
        "id": "P7zLtkMFrMJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "W6mGRzjEc-3F"
      },
      "outputs": [],
      "source": [
        "def scrape(link,filename):\n",
        "    page = requests.get(link)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    intro_text = \"\"\n",
        "    for div in soup.find_all('div', class_='field field--name-field-intro-section-one'):\n",
        "        intro_text += div.text\n",
        "        break\n",
        "\n",
        "\n",
        "    program_highlight =\"\"\n",
        "\n",
        "\n",
        "    program_highlight =scrape_part(link,'field field--name-field-intro-section-two')\n",
        "\n",
        "    if program_highlight=='':\n",
        "      for div in soup.find_all('div', class_='field field--name-field-intro-section-two'):\n",
        "        program_highlight += div.text\n",
        "        break\n",
        "      # Find the div containing the program highlights\n",
        "      div_element = soup.find('div', class_=\"field--name-field-bullet-row\")\n",
        "\n",
        "      # Find the header\n",
        "      header = div_element.find_previous_sibling('h3')\n",
        "\n",
        "      # Append the header text to the result\n",
        "      program_highlight +='\\n'+ header.get_text() + ':\\n\\n'\n",
        "\n",
        "      # Find all the <p> tags within the div\n",
        "      paragraphs = div_element.find_all('p')\n",
        "\n",
        "      # Extract the text content of each <p> tag and append to the result\n",
        "      for p in paragraphs:\n",
        "        program_highlight += p.get_text() + '\\n'\n",
        "\n",
        "    rest_text  =scrape_part(link,'field field--name-field-kicker-body')\n",
        "\n",
        "    if rest_text=='':\n",
        "      for div in soup.find_all('div', class_='field field--name-field-kicker-body'):\n",
        "        rest_text += div.text\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(intro_text)\n",
        "        f.write(\"\\n\")\n",
        "        f.write(program_highlight)\n",
        "        f.write(\"\\n\")\n",
        "        f.write(rest_text)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages=[\"https://www.yu.edu/katz/ai\",'https://www.yu.edu/katz/biotech','https://www.yu.edu/katz/cyber',\n",
        "       'https://www.yu.edu/katz/data-analytics','https://www.yu.edu/katz/digital-marketing-media','https://www.yu.edu/katz/slp',\n",
        "       'https://www.yu.edu/katz/physics','https://www.yu.edu/katz/physician-assistant','https://www.yu.edu/katz/occupational-therapy',\n",
        "       'https://www.yu.edu/katz/math-phd','https://www.yu.edu/katz/math-ma',]\n"
      ],
      "metadata": {
        "id": "HkBq_suDcGaF"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "aeJqN5_tc-3G"
      },
      "outputs": [],
      "source": [
        "for link in pages:\n",
        "\n",
        "    scrape(link,link[link.rfind('/')+1:]+'.txt')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
